# Manfacter
Investigación encargada por Alberto López de la Torre

Geneeración de presupuestos de mecanizados.

Iteración LLM con diseño de modelo 3D.

Sugerencias de la primera entrevista:
- Transladar el problema a OpenScad ya que el cambio de atributos de un modelo STL conduce a ambiguedades.
- Usar tecnología antiuga, no LLM: https://www.cs.us.es/~jalonso/cursos/d-pl/temas/tema-5.pdf 
- Usar Deep Prog Log si hay integración lógica redes neuronales.

Trabajos por parte de Alberto:
- Hacer una experiencia de diseño, en el que se simula a una charla y un diseño 3D.

# Estudio en profundidad de ChatGPT

Comenzamos estudiando el estado del arte.

Prompt inicial:

Quiero experiencias de LLM usados para a partir de descripciones manipular y construir objetos 3D. En concreto me interesan descripción del experimento, tecnología usado y usabilidad, así como negocio

¿Podrías precisar si te interesan únicamente los modelos de lenguaje grandes (LLM) utilizados directamente para generar o manipular objetos 3D (por ejemplo, en texto a 3D, edición 3D con lenguaje natural), o también te interesan herramientas híbridas que usan LLM junto con otros modelos como generadores 3D (como NeRFs o modelos CAD)?

Además, ¿quieres enfocarte en casos de uso de investigación académica, productos comerciales, o ambos? Y por último, ¿te interesa alguna industria específica (como videojuegos, arquitectura, diseño de producto, etc.)?

si NeRFs o modelos CAD. productos comerciales e investigaciones, diseño de producto

Perfecto. Investigaré experiencias relevantes donde se han utilizado modelos de lenguaje grandes (LLM) para manipular o construir objetos 3D a partir de descripciones en lenguaje natural. Me enfocaré en tecnologías como NeRFs o modelos CAD, cubriendo tanto productos comerciales como investigaciones, especialmente en el contexto de diseño de producto. También incluiré detalles sobre cada experimento o producto: descripción, tecnologías utilizadas, usabilidad y enfoque de negocio.

Te aviso en cuanto tenga todo listo.

# Uso de LLM para Generar y Manipular Objetos 3D desde Descripción Natural en Diseño de Producto

En los últimos años han surgido **experimentos académicos y soluciones comerciales** que permiten **crear o modificar modelos 3D mediante descripciones en lenguaje natural**. Estas herramientas combinan **modelos de lenguaje grandes (LLM)** con técnicas de gráficos 3D (desde *Neural Radiance Fields* – *NeRF* – hasta CAD paramétrico, funciones de distancia con signo – *SDF* –, o *diffusion models*) para facilitar el **diseño de productos** a partir de instrucciones textuales. A continuación se presentan ejemplos recientes (2022-2024) organizados en dos categorías, con detalles sobre su descripción, tecnología, usabilidad y perspectiva de negocio. Se incluye al final una tabla comparativa destacando diferencias entre enfoques académicos y comerciales.

## Investigaciones académicas destacadas (2022–2024)

### DreamFusion (Google Research, 2022) – Texto a 3D con Difusión y NeRF  
- **Descripción:** DreamFusion es un método pionero de Google Research para **generar modelos 3D a partir de descripciones de texto** ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=The%20approach%2C%20which%20combines%20a,as%20base%20meshes%20for%20sculpting)). Dada una frase como *“Una escultura metálica detallada de una ardilla con kimono tocando saxofón”*, el sistema produce un modelo 3D texturizado coherente con esa descripción ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=DreamFusion%20turns%20text%20descriptions%20into,a%20kimono%20playing%20the%20saxophone%E2%80%99)).  
- **Tecnología:** Combina un modelo de difusión *texto a imagen* pre-entrenado (Imagen de Google) con la optimización de un *Neural Radiance Field* (NeRF). En esencia, **usa las imágenes sintéticas generadas por difusión 2D como guía para optimizar una representación 3D** (tipo NeRF) que, al ser renderizada desde distintos ángulos, coincide con la descripción ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=Combining%20Neural%20Radiance%20Fields%20and,Radiance%20Fields%20and%202D%20diffusion)) ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=In%20this%20case%2C%20the%202D,from%20the%20original%20text%20description)). No necesita datos de entrenamiento 3D, aprovechando los difusores 2D pre-entrenados como prior. El resultado es un NeRF que puede exportarse a malla mediante *marching cubes* ([DreamFusion: Text-to-3D using 2D Diffusion](https://dreamfusion3d.github.io/#:~:text=Mesh%20exports)).  
- **Nivel de usabilidad:** Actualmente es un **demo de investigación**. En 2022 solo permitía usar prompts predefinidos en una galería web, sin opción de texto libre ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=At%20the%20minute%2C%20the%20opportunities,with%20DreamFusion%20are%20fairly%20limited)). Generar un modelo toma del orden de horas y requiere GPUs. La **calidad** de los modelos es adecuada como **bocetos 3D** (por ejemplo, para AR o base para escultura digital) ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=The%20approach%2C%20which%20combines%20a,as%20base%20meshes%20for%20sculpting)), pero con resolución limitada. No es una herramienta “lista para producción” aún, sino una prueba de concepto que mostró el camino hacia herramientas más prácticas.  
- **Perspectiva comercial:** DreamFusion demostró que es posible un **“texto a 3D” sin datos 3D preexistentes**, lo cual fue señalado como un posible **paso hacia herramientas de diseño masivo** ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=use%20in%20AR%20projects%2C%20or,as%20base%20meshes%20for%20sculpting)). Google lo presentó como investigación, pero su enfoque ha inspirado productos posteriores. En diseño de producto, este método sugiere poder **acelerar la generación de conceptos**: un diseñador podría obtener rápidamente un concepto 3D aproximado desde una idea escrita, para luego refinarlo. Si bien DreamFusion en sí no es comercial, **abre la puerta a futuras aplicaciones comerciales** que integren esta técnica en software de modelado o e-commerce para generar modelos de productos bajo demanda.

### Magic3D (NVIDIA, 2023) – Difusión 3D de Alta Resolución  
- **Descripción:** Magic3D es un proyecto de NVIDIA Research que mejora la velocidad y calidad de texto-a-3D respecto a DreamFusion ([AI 3D Generation Models, Applications and Tools: The Big List](https://aibusiness.com/nlp/ai-3d-generation-models-applications-and-tools-the-big-list#:~:text=Magic3D)) ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=Magic3D%20creates%20a%203D%20mesh,two%20weeks%20using%208%2C000%20models)). Dada una instrucción como *“Una rana azul venenosa sobre un lirio de agua”*, Magic3D produce un **modelo 3D con malla y texturas en unos 40 minutos** ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=Magic3D%20creates%20a%203D%20mesh,two%20weeks%20using%208%2C000%20models)). Los resultados son suficientemente detallados para usos creativos como arte digital 3D o prototipos para videojuegos. NVIDIA desarrolló Magic3D como respuesta directa al avance de Google, buscando **mayor eficiencia y resolución**.  
- **Tecnología:** Utiliza un enfoque de **difusión en dos etapas** ([AI 3D Generation Models, Applications and Tools: The Big List](https://aibusiness.com/nlp/ai-3d-generation-models-applications-and-tools-the-big-list#:~:text=Magic3D%20utilizes%20a%20two)). Primero genera un modelo 3D grueso con baja resolución usando difusión (probablemente optimizando un NeRF de baja resolución). Luego refina ese modelo mediante una representación más densa (usan una estructura de hash 3D diferenciable) para obtener la **malla final con texturas** ([AI 3D Generation Models, Applications and Tools: The Big List](https://aibusiness.com/nlp/ai-3d-generation-models-applications-and-tools-the-big-list#:~:text=,resolution%20diffusion)). Así logra **mayor calidad** que DreamFusion en la geometría y textura, y lo hace en la mitad de tiempo (≈40 min en 8 GPU vs ~1.5 h) ([Magic3D: High-Resolution Text-to-3D Content Creation | Research](https://research.nvidia.com/publication/2023-06_magic3d-high-resolution-text-3d-content-creation#:~:text=Research%20research,5%20hours%20on%20average)).  
- **Nivel de usabilidad:** Sigue siendo una **implementación de investigación**, no una herramienta con interfaz de usuario sencilla. Requiere hardware potente (GPUs) y conocimientos técnicos para ejecutar el pipeline. Sin embargo, muestra progresos importantes: modelos más detallados y tiempos de espera más razonables. Para un diseñador de productos, Magic3D podría integrarse en un futuro en flujos de trabajo para **iterar diseños complejos de forma más rápida** que la modelación manual, aunque por ahora se limita a entornos de laboratorio.  
- **Perspectiva de negocio:** Los investigadores de NVIDIA resaltan que esta tecnología permitirá **crear modelos 3D “por cualquiera” sin necesidad de entrenamiento especializado**, democratizando la creación 3D y acelerando desarrollos en videojuegos, VR, cine y publicidad ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=The%20researchers%20in%20the%20paper,%E2%80%9D)). Un caso de uso claro es **diseño conceptual rápido**: un equipo de diseño podría generar variaciones de formas orgánicas o piezas con geometrías complejas para inspirar o iniciar un proyecto, reduciendo el tiempo desde idea hasta prototipo visual. NVIDIA no comercializa Magic3D directamente, pero la investigación aporta a su ecosistema de gráficos (por ejemplo, podría integrarse en herramientas como Omniverse en el futuro). En general, Magic3D subraya el **valor añadido**: mayor velocidad en la generación de activos 3D únicos, lo que en industrias creativas y de producto puede traducirse en ahorro de costos de modelado y mayor creatividad.

### Shap-E (OpenAI, 2023) – Generación Implícita Condicionada para 3D  
- **Descripción:** Shap-E es un modelo generativo desarrollado por OpenAI capaz de **crear activos 3D a partir de texto o una imagen** ([openai/shap-e - Hugging Face](https://huggingface.co/openai/shap-e#:~:text=openai%2Fshap,Conditional%203D%20Implicit%20Functions)). Fue presentado como sucesor de un modelo previo llamado Point-E. Un ejemplo famoso es la generación de *“una silla con forma de aguacate”*, demostrando que el modelo puede capturar tanto la geometría poco convencional como texturas ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=OpenAI%20researchers%20Heewoo%20Jun%20and,a%20few%20words%20or%20descriptions)). Shap-E produce como salida una **representación 3D implícita** que puede renderizarse como mallas texturizadas o como NeRF, siendo apta incluso para **impresión 3D** tras conversión ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=intelligence%20%20model%20that%20can,a%20few%20words%20or%20descriptions)) ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=Shap,representation%20output%20space.%E2%80%99)).  
- **Tecnología:** Emplea **funciones implícitas condicionadas** por texto. A diferencia de Point-E (que generaba nubes de puntos), Shap-E genera directamente los **parámetros de una función implícita** (como SDFs o radiancia) que definen el objeto ([OpenAI Shap-E AI technology](https://lablab.ai/tech/openai/shap-e#:~:text=Shap,meshes%20and%20neural%20radiance%20fields)). Internamente utiliza un proceso similar a difusión, pero en lugar de difundir píxeles, difunde en el espacio de parámetros 3D ([OpenAI Shap-E AI technology](https://lablab.ai/tech/openai/shap-e#:~:text=OpenAI%20Shap,meshes%20and%20neural%20radiance%20fields)). Está entrenado con millones de formas 3D con sus descripciones, permitiéndole manejar **formas complejas y texturas detalladas** ([OpenAI Releases New Text-to-3D Model Shap-E - Voicebot.ai](https://voicebot.ai/2023/05/08/openai-releases-new-text-to-3d-model-shap-e/#:~:text=OpenAI%20Releases%20New%20Text,E%20produces%203D)). OpenAI reporta que Shap-E **converge más rápido** y alcanza calidad igual o mejor que Point-E ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=images%20from%20text%20captions%2C%20with,representation%20output%20space.%E2%80%99)), pese a generar en un espacio de dimensionalidad mayor.  
- **Nivel de usabilidad:** OpenAI liberó Shap-E como código abierto para la comunidad, pero **no tiene interfaz gráfica oficial**. Usuarios técnicos pueden ejecutarlo (vía Python) para obtener modelos a partir de texto. La **usabilidad** para diseñadores no programadores aún es limitada, aunque la comunidad ha creado demos y plugins (por ejemplo, integración con Blender o interfaces web simples). En términos de resultados, Shap-E ofrece **modelos más detallados y utilizables** que anteriores métodos: por ejemplo, las mallas generadas incluyen color y pueden ser impresas en 3D ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=intelligence%20%20model%20that%20can,a%20few%20words%20or%20descriptions)), lo que es valioso en prototipado rápido de producto. Se debe tener cautela, eso sí, ya que los autores advierten que sin validación humana, un objeto generado podría no cumplir exactamente la función o dimensiones deseadas ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=While%20there%20is%20a%20possibility,desired%20prompt%2C%E2%80%99%20the%20researchers%20add)).  
- **Perspectiva de negocio:** Shap-E no es un producto comercial per se, pero **OpenAI mostró con él un caso de uso de LLM + generación 3D aplicable a la industria**. Por ejemplo, un diseñador podría pedir “un envase con forma ergonómica inspirada en una gota de agua” y Shap-E generaría un modelo inicial que luego se refina manualmente. La posibilidad de **prototipar físicamente (impresión 3D)** a partir del modelo genera valor añadido en **diseño industrial** ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=intelligence%20%20model%20that%20can,a%20few%20words%20or%20descriptions)), reduciendo la brecha entre idea escrita y objeto tangible. OpenAI insinuó aplicaciones futuras en fabricación digital, aunque también señaló la necesidad de verificar que los objetos generados cumplan requisitos técnicos antes de producción ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=While%20there%20is%20a%20possibility,desired%20prompt%2C%E2%80%99%20the%20researchers%20add)). En síntesis, Shap-E avanza la viabilidad comercial de “texto a 3D” al producir modelos más directamente útiles, lo cual puede derivar en servicios o software para **creativos, estudios de diseño y fabricantes** que necesiten acelerar su flujo de desarrollo de productos.

### 3D-GPT (ANU & Univ. Oxford, 2023) – Modelado 3D Procedural Asistido por LLM  
- **Descripción:** 3D-GPT es un marco propuesto por investigadores de la Universidad Nacional de Australia (ANU) y colaboradores que utiliza un **LLM como orquestador de la generación 3D** a partir de instrucciones en lenguaje natural ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=To%20reduce%20workload%2C%20we%20introduce,with%203D%20software%20for%20asset)). El enfoque se basa en *modelado procedural*: en lugar de generar una forma de golpe, el sistema divide la tarea en pasos lógicos (como haría un modelador humano con un script). Por ejemplo, ante la solicitud *“Diseña una escena con una mesa y varias sillas alrededor”*, 3D-GPT desglosaría: (1) crear una mesa con dimensiones apropiadas, (2) crear una silla y replicarla varias veces posicionándolas alrededor de la mesa, etc., todo guiado por texto.  
- **Tecnología:** Combina un **LLM de gran tamaño con varios agentes especializados** ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=To%20reduce%20workload%2C%20we%20introduce,with%203D%20software%20for%20asset)). En su arquitectura hay un “agente despachador de tareas”, un “agente de conceptualización” y un “agente de modelado” ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=integrates%20three%20core%20agents%3A%20the,Furthermore%2C%20it%20seamlessly%20integrates%20with)). El LLM (por ejemplo, GPT-4) entiende la instrucción inicial y la **enriquece con detalles** (como dimensiones por defecto, estilos) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=integrates%20three%20core%20agents%3A%20the,Furthermore%2C%20it%20seamlessly%20integrates%20with)). Luego extrae parámetros clave de ese texto enriquecido y los pasa al agente de modelado, que **genera código o comandos para un software 3D (Blender)** a fin de construir la escena ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=First%2C%20it%20enhances%20concise%20initial,a%20basic%20framework%20for%20future)). 3D-GPT está integrado con Blender, usando su API para crear geometrías primitivas, transformarlas y aplicar materiales según las indicaciones ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=detailed%20forms%20while%20dynamically%20adapting,a%20basic%20framework%20for%20future)) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=creation,a%20basic%20framework%20for%20future)). El LLM también maneja interacciones iterativas: el usuario puede dar instrucciones adicionales (“ahora hazlas más altas las sillas”) y el sistema ajusta la escena en consecuencia. Los experimentos muestran que 3D-GPT **interpreta y ejecuta instrucciones de manera fiable** colaborando con el diseñador (quien puede refinar con lenguaje) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=creation,a%20basic%20framework%20for%20future)).  
- **Nivel de usabilidad:** Es un prototipo de investigación, pero apunta a una **interfaz natural para modelado 3D**. Un usuario con acceso al sistema podría describir la escena deseada en texto y dejar que la IA la construya en Blender, lo que **simplifica la curva de aprendizaje** para no expertos ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=To%20reduce%20workload%2C%20we%20introduce,with%203D%20software%20for%20asset)) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=In%20pursuit%20of%20this%20vision%2C,of%203D%20conceptualization%20and%203D)). Requiere el entorno Blender y el modelo de lenguaje funcionando conjuntamente, por lo que técnicamente no es trivial de montar para cualquiera hoy. Sin embargo, demuestra un **uso en tiempo real**: integrando un LLM dentro de una herramienta de modelado interactivo (Blender), pudiendo recibir instrucciones sucesivas para modificar el resultado. Esto sugiere una buena usabilidad futura: en lugar de manipular objetos con mouse y menús complejos, un diseñador podría *conversar* con la escena 3D.  
- **Perspectiva de negocio:** 3D-GPT evidencia el potencial de los LLM como **asistentes de diseño 3D**. En un contexto industrial, tal sistema podría acelerar la creación de **escenarios virtuales, prototipos de productos o configuraciones personalizadas** mediante lenguaje natural, ahorrando tiempo a ingenieros y artistas. Por ejemplo, en diseño de interiores (parte del diseño de producto de mobiliario), un profesional puede describir una disposición de sala y el sistema genera la base 3D para luego pulir detalles manualmente. El marco aún no es un producto comercial, pero podría inspirar funciones en software CAD: imaginemos un **“Copiloto” de Autodesk 3ds Max o SolidWorks** que a partir de texto genere geometrías paramétricas. Los investigadores enfatizan que 3D-GPT sienta una base para **futuros avances en generación de escenas y animación mediante LLM** ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=effectively%20with%20human%20designers,in%20scene%20generation%20and%20animation)). En cuanto a modelo de negocio, una implementación podría licenciarse a empresas de software 3D o venderse como servicio para estudios pequeños, aportando valor en términos de **productividad y accesibilidad** (menos necesidad de expertos en modelado manual para conceptos iniciales).

### LLM para Modelado CAD Paramétrico (Zhejiang University, 2024) – Precisión Industrial por Síntesis de Programas  
- **Descripción:** Un trabajo muy reciente (enero 2024) de la Universidad de Zhejiang explora si un **LLM (como GPT-4)** puede **generar modelos 3D con características geométricas precisas** y control paramétrico, a partir de descripciones de texto ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=remains%20a%20significant%20challenge%20to,Our%20work%20highlights)). Se centra en casos de *diseño industrial*, donde importan las medidas exactas y bordes afilados (por ejemplo, “un tornillo M6x20 con cabeza hexagonal” o “una carcasa rectangular de 10x5x3 cm con agujero cilíndrico central”). La investigación introduce una plataforma (y dataset llamado “PMoIS”) donde, dado un texto con especificaciones, el LLM produce un **programa de modelado 3D** que, al ejecutarse, construye el modelo solicitado ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=remains%20a%20significant%20challenge%20to,Our%20work%20highlights)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=This%20work%20explores%20a%20novel,is%20advantageous%20for%20two%20reasons)).  
- **Tecnología:** El enfoque es **LLM + síntesis de código CAD**. En lugar de generar una malla o voxel, el LLM escribe código en el lenguaje de un software CAD (por ejemplo, Python para Blender o scripts OpenSCAD). Este código usa operaciones geométricas primitivas (crear cubos, cilindros, extrusiones, etc.) con dimensiones exactas y luego los ensambla con transformaciones para obtener el objeto final ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=described%20in%20the%20prompt,These%20variables%20allow%20for)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=description.%20The%20LLM,and%20refine%20the%20generated%20code)). Por ejemplo, ante *“un prisma rectangular de 100×50×20 mm con un agujero roscado de 10 mm de diámetro”*, el LLM podría generar un script que define un cubo con esas dimensiones y resta un cilindro de diámetro 10 mm. El sistema incorpora un bucle de **auto-corrección visual**: el código generado se renderiza (por ejemplo, en Blender) y la imagen resultante se reintroduce en GPT-4 con visión para detectar desviaciones y corregir el código ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=match%20at%20L526%20description,and%20refine%20the%20generated%20code)). Esto mejora la precisión, permitiendo al LLM ajustar su salida hasta cumplir la descripción al pie de la letra. El resultado son **modelos 3D con aristas definidas, proporciones exactas y totalmente editables paramétricamente**, algo que los métodos implícitos/difusivos no logran fácilmente ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=engineering%20semantics%20%28e,equidistant%20edges%20remains%20a%20challenge)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=parametric%20controls,advantageous%20for%20two%20reasons)).  
- **Nivel de usabilidad:** Si bien es un sistema experimental, su propuesta acerca la **NLP al entorno CAD profesional**. En la práctica, un ingeniero podría describir en lenguaje natural la pieza que necesita y obtener un fichero CAD editable. Actualmente, requiere acceso a un LLM avanzado (GPT-4) y entorno de modelado (Blender) con el pipeline configurado, lo cual es complejo para un usuario común. Pero es fácil imaginar esta capacidad integrada en software CAD comercial: una **interfaz conversacional dentro de un programa CAD** donde pides “haz este borde redondeado con radio 5 mm” y la IA lo hace. **Usabilidad**: sería alta para usuarios finales si se implementa, ya que **reduce la necesidad de conocer comandos CAD específicos**, haciendo el diseño más accesible. La investigación demostró que los LLM actuales aún cometen errores lógicos o geométricos, pero con *prompt engineering* y retroalimentación visual lograron mejorar la fidelidad ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=industrial%20design%20and%20manufacturing,parametric%20modeling%20for%20industrial%20applications)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=match%20at%20L526%20description,and%20refine%20the%20generated%20code)).  
- **Perspectiva de negocio:** El valor comercial de este enfoque es claro en **sectores de ingeniería, fabricación y producto**. Permitir diseñar mediante lenguaje puede **acortar tiempos de desarrollo** y **disminuir barreras técnicas**. Un posible modelo de negocio sería incorporar esta tecnología en paquetes CAD de alto nivel (AutoCAD, SolidWorks, Fusion 360) como asistente premium. También podría existir como servicio en la nube: el cliente describe la pieza y recibe el modelo 3D descargable. Si bien todavía presenta limitaciones (el artículo menciona desafíos en razonamiento espacial complejo ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=Despite%20its%20potential%2C%20this%20approach,from%20brief%20natural%20language%20descriptions))), marca un camino para herramientas comerciales: por ejemplo, generar piezas mecánicas a medida en plataformas de impresión 3D online con solo describirlas. Además, en contexto de diseño de producto, un **diseñador industrial** podría concentrarse en la intención conceptual en palabras mientras la IA se encarga de los detalles constructivos, lo que aumenta la productividad. Esta investigación resalta tanto el **potencial** (crear objetos 3D bien definidos desde texto) como las **limitaciones actuales** de los LLM en esta tarea ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=industrial%20design%20and%20manufacturing,parametric%20modeling%20for%20industrial%20applications)), pero su evolución podría traducirse en soluciones comerciales robustas en próximos años.

### LLaMA-Mesh (NVIDIA, 2024) – Unificación de Texto y Malla 3D en un Modelo de Lenguaje  
- **Descripción:** LLaMA-Mesh es un avance investigativo de NVIDIA que demuestra por primera vez que un **modelo de lenguaje grande puede generar directamente contenido 3D** en formato de malla ([LLaMA-Mesh: NVIDIA’s Breakthrough in Unifying 3D Mesh Generation and Language Models - InfoQ](https://www.infoq.com/news/2025/01/llama-mesh-nvidia/#:~:text=NVIDIA%20researchers%20have%20introduced%20LLaMA,of%20spatial%20and%20textual%20information)). La idea central es “hablar 3D en el lenguaje de los LLM”: se formatean los datos de un objeto 3D (vértices y caras de una malla) como si fueran texto, de modo que un LLM entrenado pueda producir esa secuencia de caracteres que representa la geometría ([LLaMA-Mesh: NVIDIA’s Breakthrough in Unifying 3D Mesh Generation and Language Models - InfoQ](https://www.infoq.com/news/2025/01/llama-mesh-nvidia/#:~:text=and%20interpret%203D%20mesh%20data,of%20spatial%20and%20textual%20information)). En pruebas, LLaMA-Mesh (basado en LLaMA-65B ajustado) es capaz de, dado un prompt como *“un banco de trabajo 3D”*, **escribir un fichero OBJ completo** con vértices y caras que corresponden a un banco de trabajo ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=The%20following%20meshes%20are%20generated,caption%7D%E2%80%9D)) ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=3D%20rendering%20of%20a%20model,chair)). Igualmente, puede leer una malla y responder preguntas sobre ella en lenguaje natural, mostrando comprensión estructural.  
- **Tecnología:** Para lograr esto, NVIDIA desarrolló un método de **tokenización de mallas 3D en texto plano** ([LLaMA-Mesh: NVIDIA’s Breakthrough in Unifying 3D Mesh Generation and Language Models - InfoQ](https://www.infoq.com/news/2025/01/llama-mesh-nvidia/#:~:text=and%20interpret%203D%20mesh%20data,of%20spatial%20and%20textual%20information)). Cada coordenada numérica de vértice y cada índice de cara se convierten a texto (por ejemplo, un vértice (0.1, 0.2, 0.3) se representa como la secuencia de caracteres “0.1 0.2 0.3”). Mediante *fine-tuning* supervisado, el LLM aprende a **asociar descripciones textuales con estas representaciones de malla** y a generar ambas de forma intercalada ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=This%20work%20explores%20expanding%20the,We%20construct%20a)) ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=supervised%20fine,maintaining%20strong%20text%20generation%20performance)). Es esencialmente un enfoque multimodal unificado: un único modelo maneja texto descriptivo y geometría como un solo flujo de datos. Tras el entrenamiento, LLaMA-Mesh puede: 1) **Generar modelos 3D (malla) a partir de texto**; 2) Insertar comentarios en texto dentro de un archivo 3D (mezclando descripción y geometría); 3) **Interpretar una malla existente y responder preguntas o hacer modificaciones** ([LLaMA-Mesh: NVIDIA’s Breakthrough in Unifying 3D Mesh Generation and Language Models - InfoQ](https://www.infoq.com/news/2025/01/llama-mesh-nvidia/#:~:text=The%20team%20constructed%20a%20supervised,dataset%20allows%20the%20model%20to)). Los resultados muestran calidad comparable a métodos de generación 3D dedicados (como difusores 3D), logrando mallas coherentes y con detalles afilados ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=demonstrate%20that%20LLMs%20can%20be,maintaining%20strong%20text%20generation%20performance)). Además, el LLM conserva sus habilidades lingüísticas generales, pudiendo mantener un diálogo.  
- **Nivel de usabilidad:** Actualmente LLaMA-Mesh es un *research prototype*, no expuesto como herramienta lista. Requeriría cargar un modelo LLM de gran tamaño ajustado, lo que conlleva recursos significativos. Sin embargo, su **concepto de usabilidad es revolucionario**: uno podría interactuar en lenguaje natural y el modelo no solo “entiende” sino que **devuelve objetos 3D listos**. Imaginemos una interfaz tipo chat donde el usuario dice *“Crea una malla de un martillo de juguete”* y el chatbot responde adjuntando un modelo 3D que cumple eso. LLaMA-Mesh allana el camino para ese tipo de interfaz unificada. Desde la perspectiva de un diseñador de producto, la **interfaz sería simplemente texto** (o voz), y la salida se puede cargar en el software 3D de preferencia. Aún falta optimizar para que un usuario sin conocimientos pueda aprovecharlo (p. ej., integrar en una aplicación con botón “generar modelo”), pero la investigación ya demostró casos de uso conversacionales con éxito.  
- **Perspectiva de negocio:** LLaMA-Mesh sugiere un futuro donde las **fronteras entre describir y modelar se difuminan**. Un modelo así podría impulsar **asistentes de diseño 3D conversacionales** en entornos de colaboración o plataformas en línea. Por ejemplo, un sitio web de e-commerce custom podría tener un asistente que genere modelos de producto en tiempo real según las especificaciones del cliente (colores, dimensiones, estilo) descritas en texto. En términos de modelo de negocio, esto podría licenciarse como API de generación 3D bajo demanda. También es relevante para **realidad virtual y metaverso**: creadores de mundos virtuales podrían simplemente describir escenas o objetos y obtenerlos inmediatamente. NVIDIA destaca que integrar lenguaje y 3D permite **interacción conversacional sobre modelos 3D** ([ LLaMA-Mesh ](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/#:~:text=This%20work%20explores%20expanding%20the,We%20construct%20a)), lo cual aporta valor en revisión de diseños (un equipo podría preguntarle al modelo “¿cuál es la altura de este objeto?” o “gira la pieza 45 grados” y obtener respuesta/acción inmediata). A nivel industrial, esto podría **agilizar el ciclo de diseño**: menos iteraciones manuales, más generación asistida. Es una línea muy nueva, por lo que su adopción comercial dependerá de refinar eficiencia (procesar mallas grandes es costoso como texto) y garantizar exactitud. No obstante, demuestra un **alto potencial de valor añadido**: unificar texto y 3D permite a los LLM actuar no solo como generadores, sino también como “entendedores” de objetos, beneficiando flujos de trabajo de ingeniería y diseño (verificación automática de modelos, documentación automática de CAD, etc.).

## Soluciones comerciales e industriales recientes (2022–2024)

### Masterpiece X (Masterpiece Studio, 2023) – Generador 3D con Biblioteca y Lenguaje Natural  
- **Descripción:** Masterpiece X es un producto comercial que permite **crear modelos 3D y animaciones mediante descripciones de texto**, orientado a artistas 3D, desarrolladores de juegos y creadores de contenido ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Masterpiece%20X%20uses%20natural%20language,credit%20free%20trial)). El usuario escribe una descripción (por ejemplo, “un personaje estilizado con armadura medieval roja”) y la plataforma genera un modelo 3D base que cumple esos criterios, incluyendo geometría y texturas, e incluso rigging para animación simple en algunos casos. Cuenta con una biblioteca interna de más de 100 tipos de modelos base (humanos, animales, objetos, etc.) para combinar con las indicaciones del usuario ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Masterpiece%20X%20uses%20natural%20language,credit%20free%20trial)) ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=,3D%20API%20offered)).  
- **Tecnología utilizada:** Emplea **procesamiento de lenguaje natural (NLP)** para interpretar la entrada del usuario y luego un motor de generación 3D que monta o ajusta modelos preexistentes según la solicitud ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Masterpiece%20X%20uses%20natural%20language,credit%20free%20trial)). A diferencia de los métodos puramente neuronales, Masterpiece X se apoya en su biblioteca: identifica la categoría adecuada (por ejemplo, “humano” o “arma”) y aplica atributos de estilo, tamaño, forma, textura y color especificados en el prompt ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Masterpiece%20X%20uses%20natural%20language,credit%20free%20trial)). Es probable que use técnicas de *morphing* o mezcla de modelos así como redes generativas entrenadas en cada categoría para variar detalles. También integra herramientas de animación, permitiendo generar una animación básica (por ejemplo “una caminata”) sobre el modelo creado automáticamente.  
- **Nivel de usabilidad:** **Alto**. Ofrece una interfaz gráfica amigable, donde el usuario simplemente ingresa texto y ajusta algunos parámetros opcionales mediante menús. No requiere hardware local potente ya que el procesamiento ocurre en la nube. Incluye funciones de refinamiento manual: tras la generación inicial, se puede entrar al modo de edición para esculpir o retocar. Esto la hace apta para **usuarios con poca experiencia en modelado** – pueden obtener un punto de partida rápidamente y luego modificarlo. Requisitos técnicos: solo un navegador web o app; Masterpiece X oculta la complejidad. Su enfoque basado en plantillas garantiza que el resultado sea **coherente y limpio**, aunque puede limitar la completa originalidad (funciona mejor dentro de los 100 tipos predefinidos).  
- **Perspectiva de negocio y aplicaciones:** Masterpiece Studio opera con un modelo de negocio **freemium con créditos**: ofrecen créditos gratuitos de prueba (ej. 250) y luego paquetes de pago para generaciones adicionales ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=color%20details,credit%20free%20trial)) ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Pricing)). Sus clientes son estudios pequeños de juegos, creadores independientes y departamentos de diseño que necesitan **acelerar la creación de assets 3D**. El valor añadido está en **reducir tiempos**: por ejemplo, en vez de modelar manualmente un personaje base durante días, un diseñador lo obtiene en minutos y dedica el tiempo a la personalización fina. En diseño de producto, Masterpiece X podría emplearse para generar **conceptos de prototipos**: aunque está más enfocado a personajes y objetos genéricos, la tecnología de NLP+biblioteca es aplicable a accesorios, gadgets, mobiliario, etc. También ofrecen una **API para integración** ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=color%20details,credit%20free%20trial)), abriendo la puerta a usos industriales: por ejemplo, una empresa de realidad aumentada puede integrar Masterpiece X para que comerciantes generen modelos 3D de sus productos a partir de descripciones sin saber modelar. En resumen, Masterpiece X capitaliza la intersección de LLM (para entender instrucciones ricas) y generación 3D paramétrica para ofrecer una solución comercial lista, con un claro enfoque en **usabilidad y productividad**, monetizada vía suscripción/uso.

### 3DFY.ai (Startup, 2023) – Modelos 3D de Productos Específicos desde Texto  
- **Descripción:** 3DFY.ai es una plataforma en línea que **genera modelos 3D detallados a partir de descripciones textuales**, enfocándose en **ciertas categorías de productos** ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)). A diferencia de herramientas generales, 3DFY se especializa en objetos como muebles (lámparas, sofás, mesas, cómodas) y otros artículos de interiorismo. Por ejemplo, el usuario puede pedir “un sofá de dos plazas estilo escandinavo con tapizado gris” y el sistema producirá un modelo 3D de sofá acorde, incluso rellenando detalles no mencionados (número de patas, textura de la tela) de manera inteligente ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)). Esta especialización le permite lograr un **alto nivel de detalle y realismo** en esos dominios.  
- **Tecnología utilizada:** 3DFY.ai emplea **modelos generativos entrenados específicamente por categoría de producto** ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)). Es decir, tienen un modelo (posiblemente una combinación de redes neuronales y procedimientos) entrenado con multitud de sofás, otro para lámparas, etc. Al restringir el ámbito, pueden incorporar *know-how* de diseño: por ejemplo, su modelo de “lámparas” sabe la estructura típica de una lámpara de pie vs una lámpara de techo. Esto le permite **“rellenar información faltante”** de la descripción del usuario de forma plausible ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)). Es probable que usen una mezcla de *deep learning* (por ejemplo, autoencoders 3D o diffusion models condicionadas a texto en cada categoría) junto con reglas paramétricas (dimensiones estándar, etc.) para garantizar físicamente correctos los resultados. La generación ocurre en la nube y entrega al usuario un modelo optimizado.  
- **Nivel de usabilidad:** **Alta para su propósito específico**. La interfaz está diseñada para que usuarios de e-commerce o diseñadores puedan simplemente elegir la categoría (p. ej. “sofá”) y describir brevemente. Al estar limitado a ciertos tipos, la terminología que el sistema entiende es bastante robusta (colores, estilos, materiales comunes en muebles). No se requiere instalar nada – es un servicio web. La **calidad** de los modelos es un punto fuerte: al entrenar extensivamente en cada tipo, 3DFY produce mallas **muy detalladas incluso con input de texto escueto** ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)). Esto ahorra al usuario tener que especificar todo; por ejemplo, con “cómoda vintage madera”, el modelo generado ya tendrá tiradores, vetas de madera, etc. La usabilidad se ve ligeramente limitada fuera de sus categorías soportadas: si pides algo fuera de catálogo, no lo generará. Pero dentro de su dominio, es una herramienta plug-and-play.  
- **Perspectiva de negocio:** 3DFY.ai está orientada a **industrias de muebles, decoración y comercio electrónico** que necesitan catálogos 3D. Su modelo de negocio es de pago por generación/descarga: por ejemplo, un plan puede permitir X modelos generados al mes por cierta tarifa ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=,from%20limited%20user%20inputs)). El **valor añadido** es enorme para minoristas en línea: en vez de fotografiar y escanear cada nuevo mueble, podrían generar su modelo 3D desde la ficha técnica y usarlo en visores AR/VR para clientes. También en diseño, un fabricante de muebles puede rápidamente visualizar variaciones de un concepto simplemente cambiando la descripción (dimensiones, estilo de patas, acabados) y obteniendo el modelo. Al estar centrado en productos industriales reales (no tanto fantasía), se alinea con **usos comerciales inmediatos**. Su foco en categorías cerradas indica un camino de negocio: pueden licenciar su **motor de generación específico a fabricantes** (por ejemplo, ofrecer a una marca de sofás un generador personalizado entrenado con sus diseños, para que clientes configuren su sofá ideal online). En resumen, 3DFY ejemplifica la aplicación comercial práctica de texto-a-3D: **dominio acotado, alta calidad, usabilidad sencilla, monetización B2B**.

### Sloyd AI (Sloyd, 2022–2023) – Generación Procedural de Assets 3D por Texto  
- **Descripción:** Sloyd AI es una herramienta enfocada en **desarrolladores de videojuegos y diseñadores** que genera **modelos 3D procedurales a partir de breves descripciones**. Su promesa es *“3D instantáneo para tus juegos sin necesidad de experiencia en modelado”*. Por ejemplo, un usuario puede pedir *“una casa medieval con torre”* y Sloyd armará un modelo low-poly de casa con una torre adjunta ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=1,assemble%20those%20parts%20for%20you)). O “botella de poción con tapón”, y produce un frasco 3D apropiado. Sloyd está especializado en **props, edificaciones y armas** típicas de juegos; de hecho provee también colecciones prediseñadas (como 100 modelos gratuitos de botellas, etc., que se pueden generar y personalizar) ([Free 3D Model Generator: Bottles & Perfumes Collection - Sloyd.AI](https://www.sloyd.ai/asset-galleries/bottles#:~:text=Free%203D%20Model%20Generator%3A%20Bottles,export%203D%20models%20for%20free)).  
- **Tecnología utilizada:** A diferencia de los generadores basados puramente en IA difusiva, Sloyd combina **técnicas de generación procedural clásica con NLP**. Según sus propios desarrolladores, **no “sueña” la geometría desde cero**, sino que tiene **componentes predefinidos (parts)** y **plantillas generativas (generators)** para distintas categorías ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=1,assemble%20those%20parts%20for%20you)). El flujo es: el sistema **lee el prompt**; **decide la categoría** apropiada (p. ej., “edificio” vs “arma”) ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=1,assemble%20those%20parts%20for%20you)); luego escoge un generador dentro de esa categoría (por ejemplo, “casas medievales”) que contiene reglas y piezas modulares; finalmente **selecciona y ensambla las partes** que mejor se ajustan a la descripción (ej., añade una torre, pone ventanas, etc.) ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=1,assemble%20those%20parts%20for%20you)). Este enfoque procedural garantiza que el modelo sea válido (manifold, con topología limpia) y optimizado para juegos (bajo conteo de polígonos, mapeado UV ajustado). El NLP inicial puede estar basado en un modelo tipo GPT para analizar la frase y extraer palabras clave, aunque el corazón de la generación es algorítmico (reglas escritas + algo de aprendizaje automático para correspondencias).  
- **Nivel de usabilidad:** **Muy alta para creadores de juegos/3D novices**. Sloyd ofrece una interfaz web donde uno escribe la descripción y en segundos aparece el modelo 3D que puede rotarse en el visor. Además, permite **editar con sliders** algunas propiedades (por ejemplo, ajustar altura de la torre, ancho de la casa) para refinar el resultado ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=prompting%20serves%20as%20a%20quick,and%20jump%20straight%20into%20customization)) ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=You%20can%20start%20generating%20a,vast%20world%20of%203D%20customization)). También soporta exportación directa a motores como Unity/Unreal. Al ser procedural, la generación es casi instantánea (no hay pesadas redes generativas corriendo por minutos). La calidad visual tiende a un estilo low-poly o estilizado, adecuado para prototipos y juegos indie. Una **limitación** es la flexibilidad: está acotado a los géneros soportados; pedir algo muy fuera de su rango resultaría en un ensamblaje aproximado o en nada. Pero Sloyd se está expandiendo añadiendo más “generators” (por ej., recientemente añadieron más tipos de armas). En síntesis, es **fácil de usar, rápido y consistente** dentro de su nicho. Como ellos dicen, reduce la necesidad de modelado tedioso y permite *“saltar directamente a la personalización”* ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=Prompting%20is%20the%20use%20of,and%20jump%20straight%20into%20customization)).  
- **Perspectiva de negocio:** Sloyd se comercializa con un modelo de **suscripción freemium** y apunta sobre todo al mercado de **desarrollo de videojuegos** (independientes y estudios pequeños). La propuesta de valor es **ahorrar tiempo y costo** en crear activos 3D repetitivos (casas, árboles, armas genéricas) – en lugar de comprarlos o hacerlos uno a uno, con Sloyd el desarrollador genera variaciones ilimitadas mediante texto. Esto soporta casos de uso como **procedural content generation** en juegos, o prototipado rápido de niveles. En diseño de producto, su aplicación es más indirecta, pero podría emplearse para **conceptualizar props o elementos** que formen parte de un producto o escena. Por ejemplo, en diseño de juguetería, un diseñador podría rápidamente obtener distintas armas o accesorios de juguete para inspirarse. Sloyd se distingue por **no depender de datos masivos ni GPUs**, lo que le da una ventaja comercial: costos de operación bajos y resultados controlables. Ya se posiciona como una herramienta práctica (no solo un experimento), y su negocio consiste en atraer a creadores con una versión gratuita limitada e incentivarlos a pagar por más recursos (modelos ilimitados, exportaciones comerciales) ([Sloyd Features, Pricing, and Alternatives | AI Tools](https://aitools.inc/tools/sloyd#:~:text=Sloyd%20Features%2C%20Pricing%2C%20and%20Alternatives,models%20without%20prior%203D%20experience)). El valor añadido es claro en industria de videojuegos, y la tecnología subyacente demuestra que *combinar LLM para entender intención con generación procedural determinista* puede ser una estrategia comercialmente viable para obtener **modelos 3D útiles de forma confiable y rápida**.

### Spline AI (Spline, 2023) – Generación 3D y Animaciones vía Texto en Herramienta de Diseño  
- **Descripción:** Spline es un popular software web de diseño 3D interactivo, y en 2023 añadió **Spline AI**, una funcionalidad que permite a los usuarios **crear modelos, animaciones y texturas en tiempo real mediante comandos de texto** ([AI 3D Generation Models, Applications and Tools: The Big List](https://aibusiness.com/nlp/ai-3d-generation-models-applications-and-tools-the-big-list#:~:text=Spline%20AI)). Esto convierte a Spline en un entorno donde se puede *“hablar y diseñar”*: por ejemplo, el usuario escribe *“crear un cubo rojo girando sobre su eje”* y Spline AI inserta un cubo rojo en la escena con una animación de rotación continua. O pedir *“aplicar textura de madera al objeto”* y lo hace. Es una integración de IA generativa directamente en el flujo de trabajo de diseño.  
- **Tecnología utilizada:** Spline AI combina posiblemente un **LLM** (para interpretar instrucciones complejas) con capacidades gráficas del motor de Spline. Para generar modelos no muy complejos, podría tener un repertorio de primitivas (cubos, esferas, toros) y modelos prediseñados (quizá como kitbashing) que instancia según el pedido. Para texturas, probablemente integra modelos de generación de texturas (podría ser Stable Diffusion entrenado en texturas o utiliza bancos procedurales). Dado que Spline funciona en WebGL, la IA probablemente produce parámetros o assets que el motor ya puede manejar (por ejemplo, si pides “un árbol”, quizás carga un modelo de árbol de su librería). La característica *en tiempo real* sugiere que usan **modelos ligeros o cloud computing eficiente**, ya que la respuesta es rápida. Spline AI también puede encadenar acciones: entender cuando la frase incluye múltiples órdenes (“objeto X de color Y con animación Z”) y aplicarlas todas.  
- **Nivel de usabilidad:** **Muy alto**, puesto que está dirigida a diseñadores gráficos, incluyendo no expertos en 3D. La interfaz es el propio editor de Spline potenciado con una consola de texto o comandos de voz. Esto facilita crear prototipos interactivos sin navegar todos los menús manualmente. No requiere instalación (Spline es web) y la AI está disponible para todos los usuarios (posiblemente con limitaciones en la versión gratuita). La **precisión** de los resultados depende de la demanda: para objetos complejos que no estén en la base, puede que Spline AI no pueda inventarlos (no es un generador 3D arbitrario como DreamFusion, sino más una interfaz inteligente sobre funcionalidades existentes). Por ejemplo, “un coche deportivo detallado” puede no existir en sus recursos y no lo modelará desde cero. Funciona mejor con combinaciones de elementos básicos. Aun así, **acelera enormemente** tareas comunes: en vez de colocar un objeto, posicionarlo y animarlo manualmente, un solo comando lo hace todo. Esto hace la herramienta muy atractiva para **iteración rápida en diseño de producto**: se puede conceptualizar una escena o producto simple (como mostrar un dispositivo girando, con cierto material) simplemente describiéndolo.  
- **Perspectiva comercial:** Spline es una startup con modelo freemium/plan de pago, y la integración de AI aumenta su valor diferencial frente a otras herramientas 3D. Comercialmente, la **IA atrae a más usuarios** a la plataforma Spline, quienes podrían luego convertirse en suscriptores de planes Pro para proyecto comercial, colaboración en equipo, etc. El **caso de uso** se extiende a estudios de diseño y marketing que necesitan producir visualizaciones 3D/AR ligeras rápidamente: Spline AI les reduce la necesidad de un experto 3D para cada modificación, ya que diseñadores gráficos pueden autogestionarse cambios con lenguaje natural. El valor añadido está en la **productividad y accesibilidad**: permitir que creativos 2D o UX designers trabajen con 3D sin especializarse totalmente. No está orientada a generar modelos de manufactura o de alta fidelidad técnica, sino **contenidos 3D interactivos para web, apps y presentaciones**. En contexto industrial, podría utilizarse para mockups de productos en entornos virtuales (por ejemplo, mostrar un producto en un showroom virtual, configurándolo con texto “poner producto sobre pedestal iluminado y cámara orbitando”). Spline AI, al formar parte de un producto ya comercial, ejemplifica cómo los LLM y generadores se están incorporando como **funcionalidades de valor añadido** en herramientas de diseño existentes, con un claro camino de monetización vía suscripciones y servicios premium.

### Meshcapade (Meshcapade GmbH, 2022) – Avatares Humanos 3D Personalizados con Texto e Imágenes  
- **Descripción:** Meshcapade es una empresa que ofrece soluciones de creación de **avatares humanos en 3D** mediante IA. Sus productos permiten generar un modelo 3D de cuerpo humano *realista* a partir de diversos inputs: fotos, vídeos, escaneos y también descripción textual ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=German%20AI,integration%20into%20existing%20digital%20systems)). Por ejemplo, un usuario puede especificar atributos como “hombre atlético de 1.8m, complexión delgada, postura erguida” y Meshcapade generará una malla corporal que coincide con esas características. Usos típicos incluyen moda virtual (avatares para probar ropa), simulaciones ergonómicas, animación y juegos.  
- **Tecnología utilizada:** La tecnología central de Meshcapade es su modelo estadístico de cuerpo humano **SMPL (Skinned Multi-Person Linear model)** ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=German%20AI,integration%20into%20existing%20digital%20systems)). SMPL es un modelo paramétrico entrenado con miles de escaneos corporales, que puede generar una variedad enorme de tipos de cuerpo mediante unos pocos parámetros (altura, peso, medidas, pose). Meshcapade añade capas de IA para predecir estos parámetros desde la entrada del usuario. Si la entrada es **texto**, utiliza NLP para extraer descripciones (ej. “atlético” implica cierta masa muscular, “delgado” cierto índice de masa corporal, etc.) y asignar valores a los controles del modelo. Combina esto con *machine learning* clásico para rasgos no triviales (quizá redes entrenadas correlacionando descripciones con mediciones corporales). Cuando la entrada es imagen o video, emplean visión por computador para estimar la forma directamente. La malla resultante es altamente detallada y **realista anatómicamente**, con posibilidad de aplicar diferentes texturas de piel y rigging para animación.  
- **Nivel de usabilidad:** **Elevado en su nicho (avatares humanos)**. Meshcapade ofrece una interfaz web donde se pueden cargar fotos o escribir datos, y en minutos se obtiene el modelo 3D descargable. Para empresas, brindan una **API** para integrar la funcionalidad en otras apps (por ejemplo, en un probador virtual de una tienda online) ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=German%20AI,integration%20into%20existing%20digital%20systems)). La interfaz textual probablemente es más estructurada (puede pedir llenar un formulario de medidas y descripción) ya que describir un cuerpo en prosa libre es ambiguo. Aun así, facilita la generación de avatares sin equipamiento de escaneo 3D. **Requisitos técnicos:** ninguno del lado del cliente, es un servicio cloud. La usabilidad para diseño de producto se ve en campos como confección de prendas: un diseñador puede generar un maniquí virtual con ciertas medidas target para probar ropa digitalmente.  
- **Perspectiva de negocio:** Meshcapade ya opera con un modelo **B2B**: licencian su tecnología a empresas de moda, entretenimiento y salud. Cobran planes anuales (desde €1,500 para I+D hasta €5,000 para entornos comerciales) ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Pricing)), lo que indica el valor de nicho que proporciona. El **valor añadido** es tener **avatares 3D personalizados con precisión** sin necesidad de escáner 3D o cientos de horas de modelado. En el contexto de diseño de producto, esto es aplicable a cualquier producto relacionado con el cuerpo humano (ropa, equipo deportivo, ergonomía de herramientas, realidad virtual social). Por ejemplo, fabricantes de cascos o mochilas pueden generar modelos de distintos tipos de cuerpo para verificar ajustes. Desde la óptica LLM, Meshcapade muestra que incluso con entradas textuales limitadas (edad, genero, adjetivos físicos), la IA puede crear un modelo 3D útil – aquí el LLM/NLP es un componente para interpretar lenguaje y mapearlo a un **espacio paramétrico de diseño**. Comercialmente, subraya una estrategia: **enfocarse en un vertical (cuerpo humano), perfeccionar la tecnología y venderla como servicio especializado**, en lugar de intentar ser un generador 3D general. Esto les ha permitido ya tener clientes en e-commerce (probar ropa en avatares del cliente) y en producción virtual. En suma, es un caso exitoso de aplicación comercial de IA 3D guiada por lenguaje, con un claro modelo de negocio y uso industrial actual.

## Comparativa entre enfoques académicos y soluciones comerciales

Las soluciones anteriores pueden agruparse en **investigación académica** (pruebas de concepto avanzadas) vs **productos comerciales** (herramientas enfocadas al usuario y casos de negocio). La siguiente tabla resume diferencias clave:

| **Aspecto**             | **Enfoques Académicos (LLM + 3D)**                                      | **Soluciones Comerciales**                                     |
|-------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------|
| **Objetivo principal**   | Explorar nuevas técnicas (generación 3D novedosa, combinar LLM con 3D de formas no vistas) ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=use%20in%20AR%20projects%2C%20or,as%20base%20meshes%20for%20sculpting)) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=effectively%20with%20human%20designers,in%20scene%20generation%20and%20animation)). Muchas veces prueban que “es posible” cierto logro (ej: LLM generando mallas directamente, código CAD desde texto). | Satisfacer casos de uso específicos en industrias creativas o de producto, aportando **valor práctico inmediato** (ahorro de tiempo, facilidad). Suelen enfocarse en tareas concretas: ej. generar muebles para catálogos, avatars para moda, assets para juegos. |
| **Calidad vs. flexibilidad** | Los prototipos académicos suelen ser **muy flexibles en dominio** (pueden intentar generar cualquier objeto descrito) pero la calidad/precisión puede variar. Algunos modelos implícitos producen geometrías aproximadas o sin garantías de detalle fino ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=employing%20implicit%20representations%20can%20generate,equidistant%20edges%20remains%20a%20challenge)) ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=The%20approach%2C%20which%20combines%20a,as%20base%20meshes%20for%20sculpting)). Otros, como los basados en código, logran precisión pero con limitaciones en comprensión amplia. En general, priorizan demostrar capacidades más que pulir cada resultado. | Las herramientas comerciales **restringen el alcance** deliberadamente (categorías de objetos o estilos soportados) para asegurar **calidad consistente y control**. Al operar en dominios acotados, sus modelos generan resultados limpios, optimizados y listos para uso (pero fuera de ese rango no funcionan). La flexibilidad está supeditada a casos de uso predefinidos, a cambio de fiabilidad. |
| **Requisitos técnicos**  | Altos: típicamente requieren GPUs de última generación, entornos de código, y personal con experiencia para ejecutar o adaptar los modelos ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=At%20the%20minute%2C%20the%20opportunities,with%20DreamFusion%20are%20fairly%20limited)). Interfaces rudimentarias (a veces línea de comandos o notebooks). No pensados para un diseñador final sin conocimientos técnicos. | Bajos: suelen ofrecer **interfaces gráficas o web** amigables, ocultando la complejidad. El usuario solo necesita un navegador o una aplicación. El procesamiento pesado ocurre en la nube del proveedor. Esto los hace accesibles a diseñadores, artistas o personal de marketing sin saber de IA. |
| **Interacción e integración** | En prototipos de investigación, la interacción suele ser limitada: se ingresa un prompt y se obtiene un resultado tras un tiempo considerable (minutos u horas). Poca capacidad de refinar en vivo salvo en experimentos específicos (ej: 3D-GPT permite iteración con LLM, pero es inusual) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=creation,a%20basic%20framework%20for%20future)). Integraciones con software existentes son prueba de concepto (ej: plugin experimental para Blender). | Las soluciones comerciales ofrecen **interacción fluida y en tiempo real o casi real** en flujos de trabajo existentes. Por ejemplo, Spline AI integrada en un editor 3D interactivo, o Masterpiece X con herramientas de edición post-generación. También suelen exponer APIs para integrarse en pipelines empresariales ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=color%20details,credit%20free%20trial)). Esto las hace encajar en procesos productivos reales de empresas (diseño, marketing, ecommerce, etc.). |
| **Madurez y disponibilidad** | Estas tecnologías se presentan en papers/arXiv; el código a veces se libera como open source (Shap-E, DreamFusion), pero vienen **sin garantías ni soporte**. Son estado del arte pero en evolución rápida. Pueden contener bugs o no escalar bien. | Productos **pulidos, con soporte al usuario, y actualizaciones**. Tienen modelos de negocio que los sustentan, por lo que la empresa corrige fallos, mejora la UX, e incluso garantiza cierto rendimiento. Están disponibles bajo demanda, generalmente mediante registro o licencia, con infraestructura escalable para muchos usuarios concurrentes. |
| **Modelo de negocio**    | N/A directamente – son investigaciones publicadas que otras entidades podrían aprovechar. El “valor” aquí es conocimiento y potencial a futuro más que ingresos inmediatos. En algunos casos, grandes corporaciones detrás (Google, NVIDIA, OpenAI) podrían incorporar los hallazgos en sus productos a largo plazo. | Claro y activo – suscripciones, créditos o licencias. Enfocados en **añadir valor tangible**: reducir costes de producción 3D, habilitar nuevas experiencias (p.ej. probadores virtuales), o aumentar la velocidad de diseño. Típicamente B2B (servicios a empresas) o prosumidores, con ROI medible en procesos de diseño/venta. |

En conclusión, **ambos mundos se alimentan mutuamente**: los avances académicos habilitan capacidades nunca antes vistas (elevando lo que será posible comercialmente en el futuro cercano), mientras que los productos comerciales se centran en **casos de uso acotados donde ya se puede aportar valor consistente hoy**, a menudo incorporando versiones simplificadas o especializadas de esas nuevas técnicas. En el contexto de *diseño de producto*, ya empezamos a ver herramientas prácticas que permiten idear y prototipar con lenguaje natural, incrementando la eficiencia y creatividad, mientras que la frontera investigativa sigue expandiendo qué tan sofisticadas y precisas pueden llegar a ser estas interacciones entre lenguaje y modelado 3D ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=The%20researchers%20in%20the%20paper,%E2%80%9D)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=industrial%20design%20and%20manufacturing,parametric%20modeling%20for%20industrial%20applications)). 

**Referencias:** Las descripciones y datos provistos se basan en publicaciones recientes de investigación (Google ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=The%20approach%2C%20which%20combines%20a,as%20base%20meshes%20for%20sculpting)) ([Google’s DreamFusion turns text into 3D models | CG Channel](https://www.cgchannel.com/2022/10/googles-dreamfusion-turns-text-into-3d-models/#:~:text=DreamFusion%20turns%20text%20descriptions%20into,a%20kimono%20playing%20the%20saxophone%E2%80%99)), NVIDIA ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=Magic3D%20creates%20a%203D%20mesh,two%20weeks%20using%208%2C000%20models)) ([Generating 3D models from text with Nvidia’s Magic3D - 3D Printing Industry](https://3dprintingindustry.com/news/generating-3d-models-from-text-with-nvidias-magic3d-220520/#:~:text=The%20researchers%20in%20the%20paper,%E2%80%9D)), OpenAI ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=intelligence%20%20model%20that%20can,a%20few%20words%20or%20descriptions)) ([from avocado chair to banana plane, OpenAI’s new tool generates text-to-3D print objects](https://www.designboom.com/technology/avocado-chair-banana-airplane-shap-e-openai-text-to-image-3d-printing-05-12-2023/#:~:text=Shap,representation%20output%20space.%E2%80%99)), ANU/Oxford ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=To%20reduce%20workload%2C%20we%20introduce,with%203D%20software%20for%20asset)) ([[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models](https://ar5iv.org/pdf/2310.12945.pdf#:~:text=creation,a%20basic%20framework%20for%20future)), Zhejiang Univ. ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=remains%20a%20significant%20challenge%20to,Our%20work%20highlights)) ([\dataset: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?](https://arxiv.org/html/2401.06437v1#:~:text=This%20work%20explores%20a%20novel,is%20advantageous%20for%20two%20reasons)), etc.) así como en documentación y reseñas de productos comerciales (Masterpiece X ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=Masterpiece%20X%20uses%20natural%20language,credit%20free%20trial)), 3DFY ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=3DFY,online%20database%20after%2072%20hours)), Sloyd ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=Prompting%20is%20the%20use%20of,and%20jump%20straight%20into%20customization)) ([How to prompt in Sloyd - Generating 3D assets](https://www.sloyd.ai/blog/how-to-prompt-in-sloyd-generating-3d-assets#:~:text=1,assemble%20those%20parts%20for%20you)), Spline ([AI 3D Generation Models, Applications and Tools: The Big List](https://aibusiness.com/nlp/ai-3d-generation-models-applications-and-tools-the-big-list#:~:text=Spline%20AI)), Meshcapade ([AI 3D Model Generation Tools for Businesses (2024) - Shopify](https://www.shopify.com/blog/ai-3d-model-generation#:~:text=German%20AI,integration%20into%20existing%20digital%20systems))). Estas fuentes han sido citadas en el texto donde correspondía para avalar cada afirmación específica.

# Te comparto observaciones.

# DreamFusion
No buscamos DreamFusion porque el modelo no es CAD y no tiene iteración en la definición del objeto.  Es verdad que este modelo incluye una idea para migrar alzado planta y perfil a un modelo. La idea consiste en generar proyecciones de piezas para luego generar el corpus de entrenamiento. Idea que no supe ver en la entrevista de trabajo inicial. Llama la atención que no sean abierto los promt, luego no se consiguió una buena experiencia de usuario quedando el producto como demo. Google tiene este modelo de negocio en mente, es una alerta para una startup, o puede ser como una oportunidad para ser vendida a multinacional. Google fotos vende las fotos impresas. 

## Magic3D. 
Vuelve a ser creativo, ideal para industria de video juegos, figuras lúdicas. No para el diseño de piezas. Me hace pensar si esta industria no es un target mas apropiado. Ya que el que pide una pieza está mas acostumbrado a programas de tipo CAD. En cambio un usuario final puede pedir una figura, colorearla, imprimirla. Por ejemplo usar el iphone para un escaneo 3D y luego conseguir la foto 3D impresa. Vi hace tiempo un video que hacía esto en Japón. Pero era un proceso de estudio. Con los iphone y una app la producción se puede abaratar. 
Riesgo a contemplar: Tiempo de respuesta elevado.
Riesgo: NVidia también persigue este objetivo.

## Shap-E
Todos los modelos cumplen el mismo patrón. Dejo de leer a partir de aquí. Quiero centrarme en modelos CAD, industriales y con experiencia interactiva. 

Aunque tengo mis dudas sobre el target de clientes y los riesgos. Decido parar para cambiar impresiones con cliente. 

## ¿El encargo se adapta a mi pefil?
- Se usa tecnología de aproximación diferenciable, esto coincide con mi especialización en el doctorado. 
- Se usa programación con GPUs, esta es otra especialidad mio.

## Palabras claves. 
NeRF, Difusión, 

# Iteración Chatgpt.

Gracias por las observaciones, son muy útiles. Para ajustar la investigación a tu perfil e intereses, ¿podrías confirmar o precisar lo siguiente?

1. ¿Quieres que me centre únicamente en experiencias que usan modelos CAD o geometría paramétrica (por ejemplo, generación de código en OpenSCAD, SolidWorks, Blender scripting), descartando enfoques creativos como DreamFusion o Magic3D?

2. ¿Debo priorizar aplicaciones con interacción iterativa (por ejemplo, asistentes que permiten modificar el diseño con nuevas instrucciones)?

3. ¿Prefieres enfoques aplicables al diseño industrial o fabricación (piezas técnicas, dispositivos, prototipos físicos)?

4. ¿Quieres que incluya una valoración del posible encaje con tu perfil técnico y profesional (uso de GPUs, optimización diferenciable, etc.) en cada caso?

5. ¿Hay algún sector industrial en particular que te interese (por ejemplo, automoción, medical devices, maquinaria)?

Con esas precisiones podré iterar y enfocar mejor la investigación.

## Respuesta de guia
generación de código es la línea. Descarta enfoques creativos. Experiencia iterativa. No hace falta valoraciones en esta etapa. Diseño industrial en general.

Mientras genero sigo viendo paper.

# Paper relevante
https://arxiv.org/html/2401.06437v1#:~:text=remains%20a%20significant%20challenge%20to,Our%20work%20highlights

Incluye un interfaz tipo Juppyter Notebook.

In this study, we have developed a pipeline designed for the generation of 3D shapes with parametric controls and engineering semantics, harnessing the power of multimodal LLMs to utilize 3D software through program synthesis. We introduced \dataset, a comprehensive dataset supported by a specialized testing program, to critically assess the capabilities of LLMs within this innovative context. Our investigation into various generative strategies has identified key techniques that significantly enhance model performance in different dimensions. We have also explored the effectiveness of a visual interface in augmenting the self-correction abilities of LLMs. Our experiments and analysis have revealed the capacities of LLMs in spatial reasoning, geometric computing, program synthesis and multimodal self-correction.

A mi entender es lo que quiere el cliente.

Mi consejo sería contactar con Zeqing Yuan

Adjunto enlace de chatgpt para ver respuesta:

https://chatgpt.com/share/67de5989-d2e4-8012-9867-521f6ee48f4b


# Tiempo de trabajo
22 Marzo 6:45 - 7:45
